"""
æˆ·å‹å¸ƒå±€ç”Ÿæˆå™¨ - æ•´åˆç‰ˆ
å¤ç”¨ç°æœ‰çš„predictor.pyä»£ç ç»“æ„ï¼Œé›†æˆä¼˜åŒ–åŠŸèƒ½
"""

from core.generator import LayoutResult, GenerationConfig
from core.evaluator import EvaluationResult
from core import LayoutEvaluator, LayoutRuleEngine, ValidationResult
from core.common import extract_json_from_text, clean_json_str, parse_layout_json
import json
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass, field
import sys
import logging
from pathlib import Path

try:
    import yaml  # type: ignore[import-not-found]
except ImportError:
    yaml = None

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

# å¯¼å…¥è¯„ä¼°å’Œè§„åˆ™æ¨¡å—ï¼ˆä¸éœ€è¦GPUï¼‰

logger = logging.getLogger(__name__)


def _resolve_qwen_vl_model_class():
    """è§£æå¯ç”¨çš„Qwen-VLæ¨¡å‹ç±»ï¼ˆå…¼å®¹ä¸åŒtransformersç‰ˆæœ¬ï¼‰"""
    try:
        from transformers import Qwen2_5_VLForConditionalGeneration as model_cls
        return model_cls, "Qwen2_5_VLForConditionalGeneration", False
    except ImportError:
        pass

    try:
        from transformers import Qwen2VLForConditionalGeneration as model_cls
        return model_cls, "Qwen2VLForConditionalGeneration", False
    except ImportError:
        pass

    try:
        from transformers import AutoModelForVision2Seq as model_cls
        return model_cls, "AutoModelForVision2Seq", True
    except ImportError:
        from transformers import AutoModelForCausalLM as model_cls
        return model_cls, "AutoModelForCausalLM", True


def _is_unknown_qwen25_arch_error(exc: Exception) -> bool:
    """æ˜¯å¦ä¸º transformers ç‰ˆæœ¬è¿‡ä½å¯¼è‡´æ— æ³•è¯†åˆ« qwen2_5_vl æ¶æ„"""
    msg = str(exc)
    return (
        "model type `qwen2_5_vl`" in msg
        and "does not recognize this architecture" in msg
    )


@dataclass
class OptimizedResult:
    """ä¼˜åŒ–åçš„ç”Ÿæˆç»“æœ"""
    layout: Dict[str, List[int]]
    raw_output: str
    score: float
    is_satisfactory: bool
    issues: List[str]
    suggestions: List[str]
    candidates_count: int = 1
    optimization_rounds: int = 0
    iteration_history: List[Dict[str, Any]] = field(default_factory=list)


class LayoutPredictor:
    """
    æˆ·å‹å¸ƒå±€é¢„æµ‹å™¨
    å¤ç”¨predictor.pyçš„ä»£ç ç»“æ„ï¼Œé›†æˆè¯„ä¼°å’Œä¼˜åŒ–åŠŸèƒ½
    """

    def __init__(
        self,
        base_model_path: str = "models/Qwen2.5-VL-7B-Instruct",
        lora_adapter_path: str = "lora_model",
        device: str = "cuda",
        use_flash_attention: bool = False,
        rules_config_path: Optional[str] = None,
        prompts_config_path: Optional[str] = None
    ):
        """
        åˆå§‹åŒ–é¢„æµ‹å™¨

        Args:
            base_model_path: åŸºç¡€æ¨¡å‹è·¯å¾„
            lora_adapter_path: LoRAé€‚é…å™¨è·¯å¾„
            device: è¿è¡Œè®¾å¤‡
            use_flash_attention: æ˜¯å¦ä½¿ç”¨Flash Attention
            rules_config_path: è§„åˆ™é…ç½®æ–‡ä»¶è·¯å¾„
            prompts_config_path: æç¤ºè¯é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.device = device
        self.base_model_path = base_model_path
        self.lora_adapter_path = lora_adapter_path
        self.use_flash_attention = use_flash_attention

        # æ¨¡å‹å’Œå¤„ç†å™¨ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰
        self.model: Any = None
        self.processor: Any = None

        # åŠ è½½é…ç½®æ–‡ä»¶
        self._project_root = Path(__file__).parent
        self.prompts_config = self._load_prompts_config(prompts_config_path)

        # è¯„ä¼°å™¨å’Œè§„åˆ™å¼•æ“ï¼ˆä½¿ç”¨é…ç½®æ–‡ä»¶ï¼‰
        rules_path = self._resolve_config_path(
            rules_config_path, "config/rules.yaml"
        )
        self.evaluator = LayoutEvaluator(rules_path)
        self.rule_engine = LayoutRuleEngine(rules_path)

        # æ˜¯å¦å·²åŠ è½½æ¨¡å‹
        self._model_loaded = False

    def _resolve_config_path(self, explicit_path: Optional[str], default_relative: str) -> Optional[str]:
        """è§£æé…ç½®æ–‡ä»¶è·¯å¾„"""
        if explicit_path and Path(explicit_path).exists():
            return explicit_path
        default_path = self._project_root / default_relative
        if default_path.exists():
            return str(default_path)
        return None

    def _load_prompts_config(self, config_path: Optional[str] = None) -> Dict[str, Any]:
        """åŠ è½½æç¤ºè¯é…ç½®"""
        path = self._resolve_config_path(config_path, "config/prompts.yaml")
        if path and yaml is not None:
            try:
                with open(path, 'r', encoding='utf-8') as f:
                    config = yaml.safe_load(f)  # type: ignore[union-attr]
                logger.info(f"å·²åŠ è½½æç¤ºè¯é…ç½®: {path}")
                return config
            except Exception as e:
                logger.warning(f"åŠ è½½æç¤ºè¯é…ç½®å¤±è´¥: {e}")
        return self._default_prompts_config()

    @staticmethod
    def _default_prompts_config() -> Dict:
        """é»˜è®¤æç¤ºè¯é…ç½®"""
        return {
            'design_constraints': (
                "è®¾è®¡çº¦æŸï¼š\n"
                "1. æ‰€æœ‰æˆ¿é—´ä¸èƒ½é‡å ï¼Œæˆ¿é—´ä¹‹é—´ä¸èƒ½æœ‰äº¤å‰åŒºåŸŸ\n"
                "2. æ‰€æœ‰æˆ¿é—´å¿…é¡»åœ¨è¾¹ç•ŒèŒƒå›´å†…ï¼Œä¸èƒ½è¶…å‡ºè¾¹ç•Œ\n"
                "3. å¨æˆ¿ä¸å«ç”Ÿé—´ä¸å®œç›´æ¥ç›¸é‚»\n"
                "4. å§å®¤åº”å°½é‡é è¿‘é‡‡å…‰é¢\n"
                "5. å®¢å…åº”æœ‰è‰¯å¥½çš„é‡‡å…‰å’Œé€šé£\n"
                "6. æˆ¿é—´å°ºå¯¸åº”ç¬¦åˆäººä½“å·¥ç¨‹å­¦æ ‡å‡†\n"
            ),
            'fix_prompt': (
                "å½“å‰å¸ƒå±€å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š\n{issues}\n\n"
                "è¯·æ ¹æ®ä»¥ä¸Šé—®é¢˜å¯¹å¸ƒå±€è¿›è¡Œä¿®æ­£ï¼Œç”Ÿæˆæ”¹è¿›åçš„æˆ¿é—´å‚æ•°ã€‚\n\n"
                "åŸæœ‰å¸ƒå±€å‚æ•°ï¼š\n```json\n{original_layout}\n```\n\n"
                "è¯·è¾“å‡ºä¿®æ­£åçš„å®Œæ•´å¸ƒå±€å‚æ•°ã€‚"
            )
        }

    def load_model(self):
        """åŠ è½½æ¨¡å‹ï¼ˆå¤ç”¨predictor.pyçš„ä»£ç ï¼‰"""
        if self._model_loaded:
            return

        import torch  # type: ignore[import-not-found]
        # type: ignore[import-not-found]
        from transformers import AutoProcessor
        from peft import PeftModel  # type: ignore[import-not-found]

        model_cls, model_cls_name, use_trust_remote_code = _resolve_qwen_vl_model_class()
        logger.info(f"ä½¿ç”¨æ¨¡å‹ç±»: {model_cls_name}")

        logger.info("æ­£åœ¨åŠ è½½åŸºç¡€æ¨¡å‹: %s", self.base_model_path)

        model_source = self.base_model_path

        load_kwargs: Dict[str, Any] = {
            "device_map": "auto",
        }
        if use_trust_remote_code:
            # å…¼å®¹æ—§ç‰ˆ transformersï¼Œé€šè¿‡ auto class + remote code åŠ è½½
            load_kwargs["trust_remote_code"] = True

        def _load_from(source: str):
            if self.use_flash_attention:
                return model_cls.from_pretrained(
                    source,
                    torch_dtype=torch.bfloat16,
                    attn_implementation="flash_attention_2",
                    **load_kwargs,
                )
            return model_cls.from_pretrained(
                source,
                torch_dtype="auto",
                low_cpu_mem_usage=True,
                **load_kwargs,
            )

        try:
            self.model = _load_from(model_source)
        except FileNotFoundError as e:
            # å¦‚æœæ˜¯æœ¬åœ°ç›®å½•ç¼ºåˆ†ç‰‡ï¼Œè‡ªåŠ¨å›é€€åˆ° HuggingFace ID è¿›è¡Œä¸‹è½½åŠ è½½
            fallback_source = "Qwen/Qwen2.5-VL-7B-Instruct"
            logger.warning(
                "æœ¬åœ°æ¨¡å‹ä¸å®Œæ•´ï¼Œè‡ªåŠ¨å›é€€åˆ°è¿œç«¯æ¨¡å‹: %sï¼ˆåŸè·¯å¾„: %sï¼‰",
                fallback_source,
                model_source,
            )
            try:
                self.model = _load_from(fallback_source)
                model_source = fallback_source
            except Exception:
                raise RuntimeError(
                    "æœ¬åœ°æ¨¡å‹ç›®å½•ä¸å®Œæ•´ï¼Œä¸”è¿œç«¯å›é€€åŠ è½½å¤±è´¥ã€‚\n"
                    f"æœ¬åœ°è·¯å¾„: {self.base_model_path}\n"
                    f"å›é€€æ¨¡å‹: {fallback_source}\n"
                    "è¯·åˆ é™¤æŸåç›®å½•åé‡è¯•ï¼Œæˆ–æ£€æŸ¥ç½‘ç»œåå†æ¬¡è¿è¡Œã€‚"
                ) from e
        except Exception as e:
            if _is_unknown_qwen25_arch_error(e):
                import transformers  # type: ignore[import-not-found]

                current_version = getattr(
                    transformers, "__version__", "unknown")
                raise RuntimeError(
                    "å½“å‰ transformers ç‰ˆæœ¬ä¸æ”¯æŒ Qwen2.5-VLï¼ˆç¼ºå°‘ qwen2_5_vl æ¶æ„ï¼‰ã€‚\n"
                    f"å½“å‰ç‰ˆæœ¬: {current_version}\n"
                    "è¯·åœ¨å½“å‰ç¯å¢ƒæ‰§è¡Œå‡çº§ï¼š\n"
                    "  pip install -U 'transformers>=4.45.0'\n"
                    "å¦‚ä»å¤±è´¥ï¼Œå†æ‰§è¡Œï¼š\n"
                    "  pip install -U qwen-vl-utils"
                ) from e
            raise

        # åŠ è½½LoRAé€‚é…å™¨
        if self.lora_adapter_path:
            logger.info("æ­£åœ¨åŠ è½½LoRAé€‚é…å™¨: %s", self.lora_adapter_path)
            try:
                import warnings
                with warnings.catch_warnings():
                    # è¿‡æ»¤ visual blocks ç¼ºå°‘ LoRA æƒé‡çš„æ— å®³è­¦å‘Š
                    warnings.filterwarnings(
                        "ignore",
                        message=".*missing adapter keys.*",
                        category=UserWarning,
                    )
                    self.model = PeftModel.from_pretrained(
                        self.model, self.lora_adapter_path)
                self.model = self.model.half()
            except ValueError as e:
                # LoRA ä¸åŸºåº§æ¨¡å‹ä¸åŒ¹é…æ—¶ï¼Œè·³è¿‡é€‚é…å™¨
                logger.warning(
                    "LoRA é€‚é…å™¨ä¸åŸºåº§æ¨¡å‹ä¸åŒ¹é…ï¼Œå·²è·³è¿‡ã€‚"
                    "åŸå› : %s",
                    e,
                )

        # åŠ è½½å¤„ç†å™¨
        # è‹¥å‘ç”Ÿäº†æœ¬åœ°->è¿œç«¯å›é€€ï¼Œå¤„ç†å™¨ä¹Ÿä½¿ç”¨åŒä¸€æ¥æº
        self.base_model_path = model_source
        self.processor = AutoProcessor.from_pretrained(
            model_source,
            use_fast=True
        )

        self._model_loaded = True
        logger.info("æ¨¡å‹åŠ è½½å®Œæˆ")

    def generate_raw(
        self,
        image_path: str,
        query: str,
        max_new_tokens: int = 512,
        temperature: float = 0.7,
        top_p: float = 0.9,
        do_sample: bool = True,
        repetition_penalty: float = 1.1
    ) -> str:
        """
        åŸå§‹ç”Ÿæˆï¼ˆå¤ç”¨gen.ipynbçš„æ¨ç†ä»£ç ï¼‰

        Args:
            image_path: å›¾ç‰‡è·¯å¾„
            query: æŸ¥è¯¢æ–‡æœ¬
            max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            temperature: æ¸©åº¦å‚æ•°
            top_p: top-pé‡‡æ ·å‚æ•°
            do_sample: æ˜¯å¦é‡‡æ ·

        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬
        """
        # ç¡®ä¿æ¨¡å‹å·²åŠ è½½
        self.load_model()

        from qwen_vl_utils import process_vision_info  # type: ignore

        # æ„å»ºæ¶ˆæ¯
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": image_path},
                    {"type": "text", "text": query},
                ],
            }
        ]

        # å‡†å¤‡è¾“å…¥
        text = self.processor.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        image_inputs, video_inputs = process_vision_info(messages)
        inputs = self.processor(
            text=[text],
            images=image_inputs,
            videos=video_inputs,
            padding=True,
            return_tensors="pt",
        )
        inputs = inputs.to(self.device)

        # ç”Ÿæˆ
        import torch  # type: ignore[import-not-found]
        with torch.no_grad():
            generated_ids = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                top_p=top_p,
                do_sample=do_sample,
                repetition_penalty=repetition_penalty
            )

        # è§£ç 
        generated_ids_trimmed = [
            out_ids[len(in_ids):]
            for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
        ]
        output_text = self.processor.batch_decode(
            generated_ids_trimmed,
            skip_special_tokens=True,
            clean_up_tokenization_spaces=False
        )

        return output_text[0] if output_text else ""

    def parse_output(self, output_text: str) -> Dict[str, List[int]]:
        """è§£ææ¨¡å‹è¾“å‡ºä¸ºå¸ƒå±€å­—å…¸ï¼ˆå§”æ‰˜ç»™ common.parse_layout_jsonï¼‰"""
        return parse_layout_json(output_text)

    @staticmethod
    def _extract_json_str(text: str) -> str:
        """ä»æ¨¡å‹è¾“å‡ºä¸­æå– JSON å­—ç¬¦ä¸²ï¼ˆå§”æ‰˜ç»™ common.extract_json_from_textï¼‰"""
        return extract_json_from_text(text) or text.strip()

    @staticmethod
    def _clean_json_str(s: str) -> str:
        """æ¸…ç† LLM å¸¸è§çš„ JSON æ ¼å¼é”™è¯¯ï¼ˆå§”æ‰˜ç»™ common.clean_json_strï¼‰"""
        return clean_json_str(s)

    @staticmethod
    def _validate_layout(data) -> Dict[str, List[int]]:
        """éªŒè¯è§£æç»“æœæ˜¯å¦ä¸ºåˆæ³•å¸ƒå±€å­—å…¸"""
        if not isinstance(data, dict):
            return {}
        layout = {}
        for k, v in data.items():
            if isinstance(v, list) and len(v) == 4:
                try:
                    layout[k] = [int(x) for x in v]
                except (ValueError, TypeError):
                    continue
        return layout

    def generate(
        self,
        image_path: str,
        query: str,
        existing_layout: Optional[Dict[str, List[int]]] = None,
        config: Optional[GenerationConfig] = None
    ) -> LayoutResult:
        """
        ç”Ÿæˆå¸ƒå±€å¹¶è§£æ

        Args:
            image_path: å›¾ç‰‡è·¯å¾„
            query: æŸ¥è¯¢æ–‡æœ¬
            existing_layout: å·²æœ‰å¸ƒå±€ï¼ˆç”¨äºè¯„ä¼°ï¼‰
            config: ç”Ÿæˆé…ç½®

        Returns:
            LayoutResult: ç”Ÿæˆç»“æœ
        """
        if config is None:
            config = GenerationConfig()

        # ç”ŸæˆåŸå§‹è¾“å‡º
        raw_output = self.generate_raw(
            image_path=image_path,
            query=query,
            max_new_tokens=config.max_new_tokens,
            temperature=config.temperature,
            top_p=config.top_p,
            do_sample=config.do_sample,
            repetition_penalty=config.repetition_penalty
        )

        # è§£æè¾“å‡º
        layout = self.parse_output(raw_output)

        # è¯„ä¼°ï¼ˆå¦‚æœæä¾›äº†å·²æœ‰å¸ƒå±€ï¼‰
        score = 0.0
        issues = []
        is_valid = bool(layout)

        if layout and existing_layout:
            eval_result = self.evaluator.evaluate(layout, existing_layout)
            score = eval_result.total_score
            issues = eval_result.issues
            is_valid = eval_result.is_valid

        return LayoutResult(
            layout=layout,
            raw_output=raw_output,
            score=score,
            is_valid=is_valid,
            issues=issues
        )

    def generate_candidates(
        self,
        image_path: str,
        query: str,
        existing_layout: Optional[Dict[str, List[int]]] = None,
        num_candidates: int = 3,
        temperatures: Optional[List[float]] = None
    ) -> List[LayoutResult]:
        """
        ç”Ÿæˆå¤šä¸ªå€™é€‰å¸ƒå±€

        Args:
            image_path: å›¾ç‰‡è·¯å¾„
            query: æŸ¥è¯¢æ–‡æœ¬
            existing_layout: å·²æœ‰å¸ƒå±€
            num_candidates: å€™é€‰æ•°é‡
            temperatures: æ¸©åº¦åˆ—è¡¨

        Returns:
            List[LayoutResult]: å€™é€‰ç»“æœåˆ—è¡¨
        """
        if temperatures is None:
            temperatures = [0.3, 0.5, 0.7, 0.85, 0.95][:num_candidates]

        candidates = []
        for temp in temperatures:
            config = GenerationConfig(temperature=temp)
            result = self.generate(
                image_path=image_path,
                query=query,
                existing_layout=existing_layout,
                config=config
            )
            candidates.append(result)

        return candidates

    def select_best(
        self,
        candidates: List[LayoutResult],
        existing_layout: Dict[str, List[int]]
    ) -> Tuple[Optional[LayoutResult], Optional[EvaluationResult]]:
        """
        ä»å€™é€‰ä¸­é€‰æ‹©æœ€ä¼˜ç»“æœ

        Args:
            candidates: å€™é€‰åˆ—è¡¨
            existing_layout: å·²æœ‰å¸ƒå±€

        Returns:
            Tuple[æœ€ä¼˜ç»“æœ, è¯„ä¼°ç»“æœ]
        """
        best_result = None
        best_eval = None
        best_score = -1

        for candidate in candidates:
            if not candidate.layout:
                continue

            eval_result = self.evaluator.evaluate(
                candidate.layout, existing_layout)

            if eval_result.total_score > best_score:
                best_score = eval_result.total_score
                best_result = candidate
                best_eval = eval_result

        return best_result, best_eval

    def generate_optimized(
        self,
        image_path: str,
        query: str,
        existing_layout: Dict[str, List[int]],
        num_candidates: int = 5,
        score_threshold: float = 85.0,
        max_iterations: int = 3,
        auto_fix: bool = True,
        improvement_threshold: float = 3.0
    ) -> OptimizedResult:
        """
        å®Œæ•´ä¼˜åŒ–ç”Ÿæˆæµç¨‹ï¼š
        å¤šå€™é€‰ç”Ÿæˆ â†’ è¯„ä¼°æ‰“åˆ† â†’ é€‰æ‹©æœ€ä¼˜ â†’ è§„åˆ™ä¿®å¤ â†’ è¯†åˆ«é—®é¢˜ â†’ 
        æ³¨å…¥é—®é¢˜åˆ°Prompt â†’ é‡æ–°ç”Ÿæˆ â†’ å¾ªç¯ç›´åˆ°æ»¡æ„

        å®ç°ä¼˜åŒ–æŠ€æœ¯æ–¹æ¡ˆä¸­çš„è¿­ä»£ä¼˜åŒ–ç­–ç•¥ï¼š
        1. å¤šæ ·æ€§ç”Ÿæˆï¼šé€šè¿‡ä¸åŒæ¸©åº¦é‡‡æ ·äº§ç”Ÿå¤šä¸ªå€™é€‰
        2. è¯„åˆ†é€‰æ‹©ï¼šå¯¹å€™é€‰è¿›è¡Œäº”ç»´åº¦è¯„ä¼°ï¼Œé€‰æ‹©æœ€ä¼˜
        3. è§„åˆ™ä¿®å¤ï¼šå¯¹æœ€ä¼˜å€™é€‰è¿›è¡Œç¡¬æ€§è§„åˆ™ä¿®å¤
        4. è¿­ä»£ä¿®æ­£ï¼šå°†æœ¬è½®é—®é¢˜æ³¨å…¥Promptï¼Œå¼•å¯¼æ¨¡å‹åœ¨ä¸‹ä¸€è½®é¿å…

        Args:
            image_path: å›¾ç‰‡è·¯å¾„
            query: åŸå§‹æŸ¥è¯¢æ–‡æœ¬
            existing_layout: å·²æœ‰å¸ƒå±€å‚æ•°
            num_candidates: æ¯è½®å€™é€‰æ•°é‡
            score_threshold: æ»¡æ„åˆ†æ•°é˜ˆå€¼ï¼ˆè¾¾åˆ°ååœæ­¢ï¼‰
            max_iterations: æœ€å¤§è¿­ä»£è½®æ•°
            auto_fix: æ˜¯å¦ä½¿ç”¨è§„åˆ™å¼•æ“è‡ªåŠ¨ä¿®å¤
            improvement_threshold: æœ€å°æ”¹è¿›é˜ˆå€¼ï¼ˆä½äºæ­¤å€¼è§†ä¸ºæ”¶æ•›ï¼‰

        Returns:
            OptimizedResult: åŒ…å«å®Œæ•´ä¼˜åŒ–å†å²çš„ç»“æœ
        """
        best_layout: Optional[Dict[str, List[int]]] = None
        best_raw_output = ""
        best_score = 0.0
        best_eval: Optional[EvaluationResult] = None
        total_candidates = 0
        history: List[Dict[str, Any]] = []

        current_query = query  # åˆå§‹æŸ¥è¯¢

        for iteration in range(max_iterations):
            iter_info = {
                'iteration': iteration + 1,
                'query_type': 'åˆå§‹æŸ¥è¯¢' if iteration == 0 else 'ä¿®æ­£æŸ¥è¯¢',
            }

            logger.info("\n" + "=" * 50)
            logger.info("ğŸ”„ ç¬¬ %d/%d è½®ä¼˜åŒ–", iteration + 1, max_iterations)
            logger.info("=" * 50)

            # ========== ç¬¬1æ­¥ï¼šå¤šå€™é€‰ç”Ÿæˆ ==========
            logger.info("  ğŸ“ ç”Ÿæˆ %d ä¸ªå€™é€‰...", num_candidates)
            candidates = self.generate_candidates(
                image_path=image_path,
                query=current_query,
                existing_layout=existing_layout,
                num_candidates=num_candidates
            )
            total_candidates += len(candidates)
            iter_info['num_candidates'] = len(candidates)

            # ========== ç¬¬2æ­¥ï¼šè¯„ä¼°æ‰“åˆ† + éªŒè¯ ==========
            logger.info("  ğŸ” è¯„ä¼°å€™é€‰ç»“æœ...")
            candidate_details = []
            for i, cand in enumerate(candidates):
                if not cand.layout:
                    logger.warning("    å€™é€‰%d: âŒ è§£æå¤±è´¥", i + 1)
                    continue

                eval_result = self.evaluator.evaluate(
                    cand.layout, existing_layout)
                validation = self.rule_engine.validate(
                    cand.layout, existing_layout)

                candidate_details.append({
                    'index': i,
                    'layout': cand.layout,
                    'raw_output': cand.raw_output,
                    'score': eval_result.total_score,
                    'evaluation': eval_result,
                    'validation': validation,
                    'is_rule_valid': validation.valid
                })

                status = "âœ…" if validation.valid else "âš ï¸"
                logger.info("    å€™é€‰%d: %s å¾—åˆ†=%.1f, è§„åˆ™é€šè¿‡=%s",
                            i + 1, status, eval_result.total_score, validation.valid)

            iter_info['num_valid'] = sum(
                1 for c in candidate_details if c['is_rule_valid']
            )

            if not candidate_details:
                logger.warning("  âš ï¸ æœ¬è½®æ— æœ‰æ•ˆå€™é€‰")
                iter_info['best_score'] = 0
                iter_info['issues'] = ['æ‰€æœ‰å€™é€‰å‡è§£æå¤±è´¥']
                history.append(iter_info)
                continue

            # ========== ç¬¬3æ­¥ï¼šä¿®å¤æ‰€æœ‰å€™é€‰ + å°ºå¯¸ä¼˜åŒ– + é€‰æœ€ä¼˜ ==========
            if auto_fix:
                logger.info("  ğŸ”§ ä¿®å¤å¹¶ä¼˜åŒ–æ‰€æœ‰å€™é€‰...")
                for c in candidate_details:
                    try:
                        cur = c['layout']
                        # è§„åˆ™ä¿®å¤ï¼ˆé‡å ã€è¶…ç•Œç­‰ç¡¬æ€§é—®é¢˜ï¼‰
                        fix_result = self.rule_engine.validate_and_fix(
                            cur, existing_layout
                        )
                        if fix_result.fixed_layout:
                            cur = fix_result.fixed_layout
                        # å°ºå¯¸ä¼˜åŒ–ï¼ˆæ»¡è¶³100%æœ€å°æ ‡å‡†ï¼Œä¸å¼•å…¥æ–°é‡å ï¼‰
                        cur = self.rule_engine.optimize_dimensions(
                            cur, existing_layout
                        )
                        # æ¿€è¿›åå¤„ç†ï¼šè¾¹ç•Œå¸é™„ + ç›¸é‚»ä¿®å¤ + å¸¦é‡å®šä½å°ºå¯¸ä¼˜åŒ–
                        cur = self.rule_engine.aggressive_post_process(
                            cur, existing_layout
                        )
                        # é‡æ–°è¯„åˆ† + é‡æ–°æ ¡éªŒè§„åˆ™
                        new_eval = self.evaluator.evaluate(
                            cur, existing_layout
                        )
                        new_validation = self.rule_engine.validate(
                            cur, existing_layout
                        )
                        # æ— æ¡ä»¶æ¥å—ä¿®å¤ç»“æœï¼ˆä¿®å¤æµç¨‹ä¸ä¼šæ¶åŒ–å¸ƒå±€ï¼‰
                        c['layout'] = cur
                        c['score'] = new_eval.total_score
                        c['evaluation'] = new_eval
                        c['is_rule_valid'] = new_validation.valid
                        # æ‰“å°ä¿®å¤åçŠ¶æ€
                        fix_status = "âœ…" if new_validation.valid else "âš ï¸"
                        logger.info("    å€™é€‰%dä¿®å¤å: %s å¾—åˆ†=%.1f, è§„åˆ™é€šè¿‡=%s%s",
                                    c['index'] +
                                    1, fix_status, new_eval.total_score,
                                    new_validation.valid,
                                    f" æ®‹ä½™è¿è§„={new_validation.hard_violations}" if not new_validation.valid else "")
                    except Exception as e:
                        logger.warning("    å€™é€‰%d ä¿®å¤å¼‚å¸¸: %s", c['index'] + 1, e)

            # é€‰æ‹©æœ€ä¼˜ï¼šä¼˜å…ˆé€‰è§„åˆ™é€šè¿‡çš„ï¼Œå…¶æ¬¡é€‰å¾—åˆ†æœ€é«˜çš„
            valid_candidates = [
                c for c in candidate_details if c['is_rule_valid']]
            if valid_candidates:
                round_best = max(valid_candidates, key=lambda x: x['score'])
            else:
                round_best = max(candidate_details, key=lambda x: x['score'])

            rule_status = "âœ…è§„åˆ™é€šè¿‡" if round_best['is_rule_valid'] else "âš ï¸è§„åˆ™æœªé€šè¿‡"
            num_valid = sum(1 for c in candidate_details if c['is_rule_valid'])
            logger.info("  ğŸ† æœ¬è½®æœ€ä¼˜: å€™é€‰%d, å¾—åˆ†=%.1f, %s (é€šè¿‡ç‡=%d/%d)",
                        round_best['index'] +
                        1, round_best['score'], rule_status,
                        num_valid, len(candidate_details))

            iter_info['best_score'] = round_best['score']
            iter_info['issues'] = round_best['evaluation'].issues

            round_layout = round_best['layout']
            round_raw = round_best['raw_output']
            round_eval = round_best['evaluation']

            # ========== ç¬¬5æ­¥ï¼šæ›´æ–°å…¨å±€æœ€ä¼˜ ==========
            if round_eval.total_score > best_score:
                improvement = round_eval.total_score - best_score
                best_layout = round_layout
                best_raw_output = round_raw
                best_score = round_eval.total_score
                best_eval = round_eval
                logger.info("  â¬†ï¸ å…¨å±€æœ€ä¼˜æ›´æ–°: %.1f (+%.1f)",
                            best_score, improvement)
                iter_info['improvement'] = improvement
            else:
                logger.info("  â¡ï¸ å…¨å±€æœ€ä¼˜æœªå˜: %.1f", best_score)
                iter_info['improvement'] = 0

            history.append(iter_info)

            # ========== ç¬¬6æ­¥ï¼šæ£€æŸ¥ç»ˆæ­¢æ¡ä»¶ ==========
            if best_score >= score_threshold:
                logger.info("  âœ… è¾¾åˆ°æ»¡æ„é˜ˆå€¼ (%.1f), åœæ­¢ä¼˜åŒ–", score_threshold)
                break

            # æ£€æŸ¥æ”¶æ•›ï¼ˆä»…åœ¨æ— å‰©ä½™é—®é¢˜æ—¶å…è®¸å› æ”¹è¿›ä¸è¶³åœæ­¢ï¼‰
            has_issues = bool(best_eval and best_eval.issues)
            if iteration > 0 and iter_info.get('improvement', 0) < improvement_threshold:
                if not has_issues:
                    logger.info("  ğŸ“‰ æ”¹è¿›å¹…åº¦ä¸è¶³ä¸”æ— å‰©ä½™é—®é¢˜, åœæ­¢ä¼˜åŒ–")
                    break
                else:
                    num_issues = len(best_eval.issues) if best_eval else 0
                    logger.info("  ğŸ“‰ æ”¹è¿›å¹…åº¦ä¸è¶³ (%.1f), ä½†ä»æœ‰ %d ä¸ªé—®é¢˜, ç»§ç»­è¿­ä»£",
                                iter_info.get('improvement', 0), num_issues)

            # ========== ç¬¬7æ­¥ï¼šæ„é€ ä¿®æ­£Prompt ==========
            if iteration < max_iterations - 1 and round_eval.issues:
                current_query = self._build_fix_query(
                    original_query=query,
                    current_layout=round_layout,
                    issues=round_eval.issues,
                    existing_layout=existing_layout
                )
                logger.info("  ğŸ“‹ å·²æ³¨å…¥ %d ä¸ªé—®é¢˜åˆ°ä¸‹ä¸€è½®Prompt", len(round_eval.issues))

        # ========== æœ€ç»ˆç»“æœ ==========
        if best_layout is None:
            return OptimizedResult(
                layout={},
                raw_output="",
                score=0,
                is_satisfactory=False,
                issues=["æ‰€æœ‰è½®æ¬¡å‡æœªç”Ÿæˆæœ‰æ•ˆå¸ƒå±€"],
                suggestions=["è¯·æ£€æŸ¥è¾“å…¥å‚æ•°å’Œå›¾ç‰‡è·¯å¾„"],
                candidates_count=total_candidates,
                optimization_rounds=len(history),
                iteration_history=history
            )

        # æœ€ç»ˆè§„åˆ™ä¿®å¤ + å°ºå¯¸ä¼˜åŒ– + æ¿€è¿›åå¤„ç†
        if auto_fix:
            final_fix = self.rule_engine.validate_and_fix(
                best_layout, existing_layout)
            if final_fix.fixed_layout:
                best_layout = final_fix.fixed_layout
            best_layout = self.rule_engine.optimize_dimensions(
                best_layout, existing_layout
            )
            # æ¿€è¿›åå¤„ç†ï¼šåå¤æ‰§è¡Œ ä¿®å¤â†’å°ºå¯¸ä¼˜åŒ–(å«é‡å®šä½)â†’ç›¸é‚»ä¿®å¤â†’è¾¹ç•Œå¸é™„
            best_layout = self.rule_engine.aggressive_post_process(
                best_layout, existing_layout, max_passes=5
            )
            best_eval = self.evaluator.evaluate(best_layout, existing_layout)

        # ç¡®ä¿ best_eval ä¸ä¸º Noneï¼ˆé€»è¾‘ä¸Šæ­¤å¤„ best_layout é None æ—¶ best_eval ä¹Ÿé Noneï¼‰
        if best_eval is None:
            best_eval = self.evaluator.evaluate(best_layout, existing_layout)

        logger.info("\n" + "=" * 50)
        logger.info("ğŸ¯ ä¼˜åŒ–å®Œæˆ!")
        logger.info("  æœ€ç»ˆå¾—åˆ†: %.1f", best_eval.total_score)
        logger.info("  æ€»å€™é€‰æ•°: %d", total_candidates)
        logger.info("  è¿­ä»£è½®æ•°: %d", len(history))
        logger.info("  æ˜¯å¦æ»¡æ„: %s", best_eval.total_score >= score_threshold)
        if best_eval.issues:
            logger.info("  å‰©ä½™é—®é¢˜: %d ä¸ª", len(best_eval.issues))
        logger.info("=" * 50)

        return OptimizedResult(
            layout=best_layout,
            raw_output=best_raw_output,
            score=best_eval.total_score,
            is_satisfactory=best_eval.total_score >= score_threshold,
            issues=best_eval.issues,
            suggestions=best_eval.suggestions,
            candidates_count=total_candidates,
            optimization_rounds=len(history),
            iteration_history=history
        )

    def _build_fix_query(
        self,
        original_query: str,
        current_layout: Dict[str, List[int]],
        issues: List[str],
        existing_layout: Optional[Dict[str, List[int]]] = None
    ) -> str:
        """
        æ„é€ è¿­ä»£ä¿®æ­£æŸ¥è¯¢ï¼šå°†è¯„ä¼°é—®é¢˜è½¬åŒ–ä¸ºå…·ä½“å¯æ“ä½œçš„æ•°å€¼ä¿®æ­£æŒ‡ä»¤ï¼Œ
        è€Œä¸åªæ˜¯ç¬¼ç»Ÿåœ°åˆ—å‡ºé—®é¢˜åã€‚

        ä¾‹å¦‚ "å®½åº¦ä¸è¶³: å®¢å… (æœ€å°3300mm)" â†’ "å®¢å…çŸ­è¾¹åªæœ‰2100mmï¼Œéœ€è¦â‰¥3300mm"
        ä¾‹å¦‚ "æˆ¿é—´é‡å : å§å®¤1 ä¸ å§å®¤2" â†’ "å§å®¤1å’Œå§å®¤2çŸ©å½¢åŒºåŸŸé‡å ï¼Œè¯·è°ƒæ•´åæ ‡ä½¿å…¶ä¸äº¤å‰"
        """
        import re

        # å°†é—®é¢˜è½¬åŒ–ä¸ºå…·ä½“ä¿®æ­£æŒ‡ä»¤
        fix_instructions = []
        for issue in issues[:6]:  # æœ€å¤š6æ¡ï¼Œé¿å…è¿‡é•¿
            if "å®½åº¦ä¸è¶³" in issue or "é•¿åº¦ä¸è¶³" in issue:
                # ä» issue æå–æˆ¿é—´åå’Œæœ€å°å€¼
                m = re.search(r'(å®½åº¦|é•¿åº¦)ä¸è¶³.*?:\s*(\S+)\s*\(æœ€å°(\d+)mm\)', issue)
                if m:
                    dim, room, min_val = m.group(1), m.group(2), m.group(3)
                    params = current_layout.get(room)
                    if params:
                        actual_short = min(params[2], params[3])
                        actual_long = max(params[2], params[3])
                        if dim == "å®½åº¦":
                            fix_instructions.append(
                                f"{room}çŸ­è¾¹={actual_short}mmä¸è¶³ï¼Œéœ€â‰¥{min_val}mm"
                            )
                        else:
                            fix_instructions.append(
                                f"{room}é•¿è¾¹={actual_long}mmä¸è¶³ï¼Œéœ€â‰¥{min_val}mm"
                            )
                    else:
                        fix_instructions.append(issue)
                else:
                    fix_instructions.append(issue)
            elif "é¢ç§¯ä¸è¶³" in issue:
                m = re.search(r'é¢ç§¯ä¸è¶³.*?:\s*(\S+)\s*\(æœ€å°([\d.]+)å¹³ç±³\)', issue)
                if m:
                    room, min_area = m.group(1), m.group(2)
                    params = current_layout.get(room)
                    if params:
                        actual = params[2] * params[3] / 1_000_000
                        fix_instructions.append(
                            f"{room}é¢ç§¯={actual:.1f}ã¡ä¸è¶³ï¼Œéœ€â‰¥{min_area}ã¡"
                        )
                    else:
                        fix_instructions.append(issue)
                else:
                    fix_instructions.append(issue)
            elif "é‡å " in issue:
                # åŒºåˆ†åŸºç¡€è®¾æ–½é‡å  vs æˆ¿é—´é—´é‡å ï¼Œç»™å‡ºå…·ä½“åæ ‡
                if "åŸºç¡€è®¾æ–½" in issue:
                    # æå–åŸºç¡€è®¾æ–½åå’Œæˆ¿é—´åï¼Œç»™å‡ºç¦åŒºèŒƒå›´
                    import re as _re
                    m = _re.search(r'(\S+)\s*ä¸\s*(\S+)', issue)
                    if m:
                        rname, iname = m.group(1), m.group(2)
                        infra_params = None
                        _el = existing_layout or {}
                        for full_key in list(_el.keys()):
                            if iname in full_key:
                                infra_params = _el.get(full_key)
                                break
                        if infra_params:
                            fix_instructions.append(
                                f"{rname}ä¸{iname}åŒºåŸŸ[{infra_params}]é‡å ï¼Œ"
                                f"è¯·å°†{rname}ç§»åˆ°è¯¥çŸ©å½¢åŒºåŸŸä¹‹å¤–"
                            )
                        else:
                            fix_instructions.append(issue + "ï¼Œè¯·å°†æˆ¿é—´ç§»åˆ°åŸºç¡€è®¾æ–½åŒºåŸŸä¹‹å¤–")
                    else:
                        fix_instructions.append(issue + "ï¼Œè¯·å°†æˆ¿é—´ç§»åˆ°åŸºç¡€è®¾æ–½åŒºåŸŸä¹‹å¤–")
                else:
                    fix_instructions.append(issue + "ï¼Œè¯·è°ƒæ•´åæ ‡ä½¿å…¶ä¸äº¤å‰")
            elif "è¶…å‡ºè¾¹ç•Œ" in issue:
                fix_instructions.append(issue + "ï¼Œè¯·ç¼©å°å°ºå¯¸æˆ–ç§»åŠ¨ä½ç½®")
            elif "é‡‡å…‰ä¸è¶³" in issue:
                fix_instructions.append(issue + "ï¼Œè¯·å°†å…¶ç§»åˆ°é è¿‘é‡‡å…‰é¢çš„ä½ç½®")
            elif "å…¥å£" in issue and "å®¢å…" in issue:
                # æä¾›å…¥å£åæ ‡å¸®åŠ©æ¨¡å‹å®šä½
                entry_params = None
                _el = existing_layout or {}
                for ek, ev in _el.items():
                    if "å…¥å£" in ek:
                        entry_params = ev
                        break
                if entry_params:
                    fix_instructions.append(
                        f"å®¢å…åº”é è¿‘ä¸»å…¥å£[{entry_params}]ï¼Œè¯·å°†å®¢å…ç§»åˆ°å…¥å£é™„è¿‘"
                    )
                else:
                    fix_instructions.append("å®¢å…åº”é è¿‘å…¥å£ï¼Œè¯·è°ƒæ•´ä½ç½®")
            elif "ä¸å®œç›¸é‚»" in issue:
                fix_instructions.append(issue + "ï¼Œè¯·æ‹‰å¼€å®ƒä»¬çš„è·ç¦»")
            else:
                fix_instructions.append(issue)

        issues_text = "ï¼›".join(fix_instructions)
        layout_json = json.dumps(current_layout, ensure_ascii=False)

        return (
            f"{original_query}\n"
            f"ä¸Šæ¬¡ç”Ÿæˆçš„ç»“æœå­˜åœ¨é—®é¢˜ï¼Œè¯·ä¿®æ­£ï¼š{issues_text}ã€‚\n"
            f"ä¸Šæ¬¡ç»“æœï¼š\n```json\n{layout_json}\n```\n"
            f"è¯·è¾“å‡ºä¿®æ­£åçš„å®Œæ•´JSONï¼Œæ ¼å¼ä¸º```json\n{{...}}\n```"
        )

    def evaluate(
        self,
        layout: Dict[str, List[int]],
        existing_layout: Dict[str, List[int]]
    ) -> EvaluationResult:
        """è¯„ä¼°å¸ƒå±€"""
        return self.evaluator.evaluate(layout, existing_layout)

    def validate(
        self,
        layout: Dict[str, List[int]],
        existing_layout: Dict[str, List[int]],
        auto_fix: bool = False
    ) -> ValidationResult:
        """éªŒè¯å¸ƒå±€"""
        if auto_fix:
            return self.rule_engine.validate_and_fix(layout, existing_layout)
        return self.rule_engine.validate(layout, existing_layout)


# ==================== æˆ¿é—´ç±»å‹ â†’ æœ€å°å°ºå¯¸æ˜ å°„ï¼ˆä¸ rules.yaml ä¸€è‡´ï¼‰ ====================
_ROOM_SIZE_SPEC = {
    "å§å®¤":   {"w": 2400, "l": 3000, "a": 7.2},
    "ä¸»å§":   {"w": 3000, "l": 3600, "a": 10.8},
    "å®¢å…":   {"w": 3300, "l": 4500, "a": 14.85},
    "å¨æˆ¿":   {"w": 1800, "l": 2400, "a": 4.32},
    "å«ç”Ÿé—´": {"w": 1500, "l": 2100, "a": 3.15},
    "ä¸»å«":   {"w": 1800, "l": 2400, "a": 4.32},
    "é¤å…":   {"w": 1500, "l": 2000, "a": 3.0},
    "å‚¨è—":   {"w": 1200, "l": 1500, "a": 1.8},
    "ç„å…³":   {"w": 1200, "l": 1500, "a": 1.8},
    "æ¥¼æ¢¯":   {"w": 2100, "l": 2400, "a": 5.04},
    "é—¨å»Š":   {"w": 1200, "l": 2400, "a": 2.88},
    "æ¬¡å…¥å£": {"w": 600,  "l": 600,  "a": 0.36},
}


def _room_type(name: str) -> str:
    """æˆ¿é—´å â†’ ç±»å‹ï¼ˆå§å®¤1â†’å§å®¤ï¼‰"""
    for t in _ROOM_SIZE_SPEC:
        if t in name:
            return t
    return name


def _build_size_constraints(rooms_to_generate: List[str]) -> str:
    """
    æ ¹æ®å¾…ç”Ÿæˆæˆ¿é—´åˆ—è¡¨ï¼ŒåŠ¨æ€æ„å»ºæœ€å°å°ºå¯¸çº¦æŸæ–‡æœ¬ã€‚
    åªåˆ—å‡ºä¸æœ¬æ¬¡ç”Ÿæˆæœ‰å…³çš„æˆ¿é—´ç±»å‹ï¼Œé¿å…å†—ä½™ã€‚
    """
    seen_types = set()
    lines = []
    for room in rooms_to_generate:
        rt = _room_type(room)
        if rt in _ROOM_SIZE_SPEC and rt not in seen_types:
            seen_types.add(rt)
            spec = _ROOM_SIZE_SPEC[rt]
            lines.append(f"{rt}: çŸ­è¾¹â‰¥{spec['w']}mm, é•¿è¾¹â‰¥{spec['l']}mm")
    return "ï¼›".join(lines)


def build_query(
    house_type: str,
    floor_type: str,
    existing_params: Dict[str, List[int]],
    rooms_to_generate: List[str],
    design_constraints: Optional[str] = None,
    prompts_config: Optional[Dict[str, Any]] = None
) -> str:
    """
    æ„å»ºæŸ¥è¯¢æ–‡æœ¬ï¼ˆå¢å¼ºç‰ˆï¼Œæ³¨å…¥é‡åŒ–çº¦æŸï¼‰

    ç­–ç•¥ï¼šä¿ç•™ä¸è®­ç»ƒæ•°æ®å®Œå…¨ä¸€è‡´çš„ä¸»ä½“æ ¼å¼ï¼ˆæ¨¡å‹ä»è¿™ä¸ªæ¨¡å¼ä¸­å­¦ä¼š
    äº† JSON è¾“å‡ºï¼‰ï¼Œåœ¨æœ«å°¾è¿½åŠ ç®€çŸ­çš„é‡åŒ–ç¡¬çº¦æŸï¼Œç”¨è‡ªç„¶è¯­è¨€å†™ï¼Œ
    ä¸ç ´åè®­ç»ƒæ¨¡å¼ã€‚
    """
    existing_json = json.dumps(existing_params, ensure_ascii=False)
    rooms_json = json.dumps(rooms_to_generate, ensure_ascii=False)

    # ä» existing_params è§£æè¾¹ç•ŒèŒƒå›´
    boundary = existing_params.get("è¾¹ç•Œ", None)

    # ---- ä¸»ä½“ï¼šä¸è®­ç»ƒæ•°æ®æ ¼å¼å®Œå…¨ä¸€è‡´ ----
    query = (
        f'è¯·æ ¹æ®è¿™å¼ å›¾ç‰‡ä¸­å·²æœ‰çš„æˆ·å‹ä¿¡æ¯ä»¥åŠå¯¹åº”çš„å‚æ•°ï¼Œå¸®æˆ‘ç”Ÿæˆå…¶ä½™æˆ¿é—´çš„å‚æ•°ï¼Œ'
        f'å¾—åˆ°ä¸€ä¸ªå®Œæ•´çš„åˆç†å¹³é¢å¸ƒå±€ã€‚æ„æˆæˆ·å‹çš„æ‰€æœ‰ç©ºé—´å•å…ƒå‡è¡¨ç¤ºä¸ºçŸ©å½¢ï¼Œ'
        f'ç”¨xè½´åæ ‡ã€yè½´åæ ‡ã€é•¿åº¦ã€å®½åº¦å››ä¸ªå‚æ•°è¡¨ç¤ºã€‚'
        f'æœ¬æˆ·å‹ä¸º"{house_type}"ä½å®…ï¼Œå›¾ç‰‡ä¸­çš„ä¸º"{floor_type}"å¹³é¢ã€‚\n'
        f'å›¾ç‰‡ä¸­å·²æœ‰ä¿¡æ¯å¯¹åº”çš„å‚æ•°ä¸ºï¼š\n'
        f'```json\n{existing_json}\n```'
        f'å…¶ä½™å¾…ç”Ÿæˆçš„"{floor_type}"æˆ¿é—´çš„åç§°ä¸ºï¼š\n'
        f'```json\n{rooms_json}```'
    )

    # ---- è¿½åŠ ï¼šç®€çŸ­é‡åŒ–çº¦æŸï¼ˆè‡ªç„¶è¯­è¨€ï¼Œä¸å½±å“ JSON è¾“å‡ºæ ¼å¼ï¼‰ ----
    constraints = []
    if boundary and len(boundary) == 4:
        bx, by, bw, bh = boundary
        constraints.append(
            f"æ‰€æœ‰æˆ¿é—´çš„xåæ ‡â‰¥{bx}ï¼Œyåæ ‡â‰¥{by}ï¼Œ"
            f"x+é•¿åº¦â‰¤{bx+bw}ï¼Œy+å®½åº¦â‰¤{by+bh}"
        )
    constraints.append("ä»»æ„ä¸¤ä¸ªæˆ¿é—´çš„çŸ©å½¢åŒºåŸŸä¸èƒ½é‡å ")

    size_text = _build_size_constraints(rooms_to_generate)
    if size_text:
        constraints.append(f"æœ€å°å°ºå¯¸è¦æ±‚ï¼š{size_text}")

    constraints.append("å¨æˆ¿ä¸å®œä¸å«ç”Ÿé—´ç›´æ¥ç›¸é‚»")
    constraints.append("å®¢å…ã€å§å®¤åº”é è¿‘é‡‡å…‰é¢")
    constraints.append("å®¢å…åº”é è¿‘ä¸»å…¥å£")
    constraints.append("é¤å…åº”ä¸å¨æˆ¿ç›¸é‚»")
    constraints.append("æˆ¿é—´åº”å°½é‡å¡«æ»¡è¾¹ç•Œç©ºé—´ï¼Œé¿å…å¤§é¢ç§¯ç©ºç™½")
    constraints.append("æˆ¿é—´é•¿å®½æ¯”ä¸å®œè¶…è¿‡4:1")
    constraints.append("æˆ¿é—´ä¸èƒ½ä¸å·²æœ‰çš„é‡‡å…‰åŒºã€é»‘ä½“åŒºã€ä¸»å…¥å£åŒºåŸŸé‡å ")
    constraints.append("åæ ‡å’Œå°ºå¯¸å–300çš„æ•´æ•°å€ï¼ˆå»ºç­‘æ¨¡æ•°å¯¹é½ï¼‰")
    constraints.append("åªè¾“å‡ºå¾…ç”Ÿæˆæˆ¿é—´çš„å‚æ•°ï¼Œä¸è¦åŒ…å«å·²æœ‰æˆ¿é—´")

    query += "\næ³¨æ„ï¼š" + "ï¼›".join(constraints) + "ã€‚"
    query += "\nè¯·ç›´æ¥è¾“å‡ºJSONï¼Œæ ¼å¼ä¸º```json\n{...}\n```ï¼ŒåªåŒ…å«å¾…ç”Ÿæˆçš„æˆ¿é—´ã€‚"

    return query


# ä¾¿æ·å‡½æ•°
def create_predictor(
    base_model_path: str = "models/Qwen2.5-VL-7B-Instruct",
    lora_adapter_path: str = "lora_model",
    **kwargs
) -> LayoutPredictor:
    """åˆ›å»ºé¢„æµ‹å™¨å®ä¾‹"""
    return LayoutPredictor(
        base_model_path=base_model_path,
        lora_adapter_path=lora_adapter_path,
        **kwargs
    )


if __name__ == "__main__":
    # æµ‹è¯•ï¼ˆä¸åŠ è½½æ¨¡å‹ï¼Œä»…æµ‹è¯•è¯„ä¼°åŠŸèƒ½ï¼‰
    logger.info("æµ‹è¯• LayoutPredictorï¼ˆè¯„ä¼°åŠŸèƒ½ï¼‰...")

    predictor = LayoutPredictor()

    # æµ‹è¯•è¯„ä¼°
    existing = {
        "è¾¹ç•Œ": [0, 0, 9600, 10500],
        "å—é‡‡å…‰": [0, -1200, 9600, 1200],
    }

    generated = {
        "å®¢å…": [0, 0, 4000, 4000],
        "å§å®¤1": [0, 4500, 3300, 4000],
        "å¨æˆ¿": [4500, 0, 2400, 3000],
    }

    result = predictor.evaluate(generated, existing)
    logger.info("è¯„ä¼°å¾—åˆ†: %.1f", result.total_score)
    logger.info("é—®é¢˜: %s", result.issues)

    # æµ‹è¯•éªŒè¯
    val_result = predictor.validate(generated, existing)
    logger.info("éªŒè¯é€šè¿‡: %s", val_result.valid)

    # æµ‹è¯•æŸ¥è¯¢æ„å»º
    query = build_query(
        house_type="åŸå¸‚",
        floor_type="ä¸€å±‚",
        existing_params=existing,
        rooms_to_generate=["å®¢å…", "å§å®¤1", "å¨æˆ¿"]
    )
    logger.info("\næ„å»ºçš„æŸ¥è¯¢:\n%s...", query[:200])

    logger.info("\næµ‹è¯•å®Œæˆ!")
