"""
æˆ·å‹å¸ƒå±€ç”Ÿæˆå™¨ - æ•´åˆç‰ˆ
å¤ç”¨ç°æœ‰çš„predictor.pyä»£ç ç»“æ„ï¼Œé›†æˆä¼˜åŒ–åŠŸèƒ½
"""

import json
import yaml
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass, field
import sys
import logging
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

# å¯¼å…¥è¯„ä¼°å’Œè§„åˆ™æ¨¡å—ï¼ˆä¸éœ€è¦GPUï¼‰
from core import LayoutEvaluator, LayoutRuleEngine, ValidationResult
from core.evaluator import EvaluationResult
from core.generator import LayoutResult, GenerationConfig

logger = logging.getLogger(__name__)


@dataclass
class OptimizedResult:
    """ä¼˜åŒ–åçš„ç”Ÿæˆç»“æœ"""
    layout: Dict[str, List[int]]
    raw_output: str
    score: float
    is_satisfactory: bool
    issues: List[str]
    suggestions: List[str]
    candidates_count: int = 1
    optimization_rounds: int = 0
    iteration_history: List[Dict[str, Any]] = field(default_factory=list)


class LayoutPredictor:
    """
    æˆ·å‹å¸ƒå±€é¢„æµ‹å™¨
    å¤ç”¨predictor.pyçš„ä»£ç ç»“æ„ï¼Œé›†æˆè¯„ä¼°å’Œä¼˜åŒ–åŠŸèƒ½
    """
    
    def __init__(
        self,
        base_model_path: str = "models/Qwen2.5-VL-7B-Instruct",
        lora_adapter_path: str = "lora_model",
        device: str = "cuda",
        use_flash_attention: bool = False,
        rules_config_path: str = None,
        prompts_config_path: str = None
    ):
        """
        åˆå§‹åŒ–é¢„æµ‹å™¨
        
        Args:
            base_model_path: åŸºç¡€æ¨¡å‹è·¯å¾„
            lora_adapter_path: LoRAé€‚é…å™¨è·¯å¾„
            device: è¿è¡Œè®¾å¤‡
            use_flash_attention: æ˜¯å¦ä½¿ç”¨Flash Attention
            rules_config_path: è§„åˆ™é…ç½®æ–‡ä»¶è·¯å¾„
            prompts_config_path: æç¤ºè¯é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.device = device
        self.base_model_path = base_model_path
        self.lora_adapter_path = lora_adapter_path
        self.use_flash_attention = use_flash_attention
        
        # æ¨¡å‹å’Œå¤„ç†å™¨ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰
        self.model = None
        self.processor = None
        
        # åŠ è½½é…ç½®æ–‡ä»¶
        self._project_root = Path(__file__).parent
        self.prompts_config = self._load_prompts_config(prompts_config_path)
        
        # è¯„ä¼°å™¨å’Œè§„åˆ™å¼•æ“ï¼ˆä½¿ç”¨é…ç½®æ–‡ä»¶ï¼‰
        rules_path = self._resolve_config_path(
            rules_config_path, "config/rules.yaml"
        )
        self.evaluator = LayoutEvaluator(rules_path)
        self.rule_engine = LayoutRuleEngine(rules_path)
        
        # æ˜¯å¦å·²åŠ è½½æ¨¡å‹
        self._model_loaded = False
    
    def _resolve_config_path(self, explicit_path: str, default_relative: str) -> Optional[str]:
        """è§£æé…ç½®æ–‡ä»¶è·¯å¾„"""
        if explicit_path and Path(explicit_path).exists():
            return explicit_path
        default_path = self._project_root / default_relative
        if default_path.exists():
            return str(default_path)
        return None
    
    def _load_prompts_config(self, config_path: str = None) -> Dict:
        """åŠ è½½æç¤ºè¯é…ç½®"""
        path = self._resolve_config_path(config_path, "config/prompts.yaml")
        if path:
            try:
                with open(path, 'r', encoding='utf-8') as f:
                    config = yaml.safe_load(f)
                logger.info(f"å·²åŠ è½½æç¤ºè¯é…ç½®: {path}")
                return config
            except Exception as e:
                logger.warning(f"åŠ è½½æç¤ºè¯é…ç½®å¤±è´¥: {e}")
        return self._default_prompts_config()
    
    @staticmethod
    def _default_prompts_config() -> Dict:
        """é»˜è®¤æç¤ºè¯é…ç½®"""
        return {
            'design_constraints': (
                "è®¾è®¡çº¦æŸï¼š\n"
                "1. æ‰€æœ‰æˆ¿é—´ä¸èƒ½é‡å ï¼Œæˆ¿é—´ä¹‹é—´ä¸èƒ½æœ‰äº¤å‰åŒºåŸŸ\n"
                "2. æ‰€æœ‰æˆ¿é—´å¿…é¡»åœ¨è¾¹ç•ŒèŒƒå›´å†…ï¼Œä¸èƒ½è¶…å‡ºè¾¹ç•Œ\n"
                "3. å¨æˆ¿ä¸å«ç”Ÿé—´ä¸å®œç›´æ¥ç›¸é‚»\n"
                "4. å§å®¤åº”å°½é‡é è¿‘é‡‡å…‰é¢\n"
                "5. å®¢å…åº”æœ‰è‰¯å¥½çš„é‡‡å…‰å’Œé€šé£\n"
                "6. æˆ¿é—´å°ºå¯¸åº”ç¬¦åˆäººä½“å·¥ç¨‹å­¦æ ‡å‡†\n"
            ),
            'fix_prompt': (
                "å½“å‰å¸ƒå±€å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š\n{issues}\n\n"
                "è¯·æ ¹æ®ä»¥ä¸Šé—®é¢˜å¯¹å¸ƒå±€è¿›è¡Œä¿®æ­£ï¼Œç”Ÿæˆæ”¹è¿›åçš„æˆ¿é—´å‚æ•°ã€‚\n\n"
                "åŸæœ‰å¸ƒå±€å‚æ•°ï¼š\n```json\n{original_layout}\n```\n\n"
                "è¯·è¾“å‡ºä¿®æ­£åçš„å®Œæ•´å¸ƒå±€å‚æ•°ã€‚"
            )
        }
    
    def load_model(self):
        """åŠ è½½æ¨¡å‹ï¼ˆå¤ç”¨predictor.pyçš„ä»£ç ï¼‰"""
        if self._model_loaded:
            return
        
        import torch
        from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor
        from peft import PeftModel
        
        print(f"æ­£åœ¨åŠ è½½åŸºç¡€æ¨¡å‹: {self.base_model_path}")
        
        if self.use_flash_attention:
            self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                self.base_model_path,
                torch_dtype=torch.bfloat16,
                attn_implementation="flash_attention_2",
                device_map="auto",
            )
        else:
            self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
                self.base_model_path,
                torch_dtype="auto",
                device_map="auto",
                low_cpu_mem_usage=True
            )
        
        # åŠ è½½LoRAé€‚é…å™¨
        if self.lora_adapter_path:
            print(f"æ­£åœ¨åŠ è½½LoRAé€‚é…å™¨: {self.lora_adapter_path}")
            self.model = PeftModel.from_pretrained(self.model, self.lora_adapter_path)
            self.model = self.model.half()
        
        # åŠ è½½å¤„ç†å™¨
        self.processor = AutoProcessor.from_pretrained(
            self.base_model_path, 
            use_fast=True
        )
        
        self._model_loaded = True
        print("æ¨¡å‹åŠ è½½å®Œæˆ")
    
    def generate_raw(
        self,
        image_path: str,
        query: str,
        max_new_tokens: int = 256,
        temperature: float = 0.7,
        top_p: float = 0.9,
        do_sample: bool = True
    ) -> str:
        """
        åŸå§‹ç”Ÿæˆï¼ˆå¤ç”¨gen.ipynbçš„æ¨ç†ä»£ç ï¼‰
        
        Args:
            image_path: å›¾ç‰‡è·¯å¾„
            query: æŸ¥è¯¢æ–‡æœ¬
            max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            temperature: æ¸©åº¦å‚æ•°
            top_p: top-pé‡‡æ ·å‚æ•°
            do_sample: æ˜¯å¦é‡‡æ ·
            
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬
        """
        # ç¡®ä¿æ¨¡å‹å·²åŠ è½½
        self.load_model()
        
        from qwen_vl_utils import process_vision_info
        
        # æ„å»ºæ¶ˆæ¯
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": image_path},
                    {"type": "text", "text": query},
                ],
            }
        ]
        
        # å‡†å¤‡è¾“å…¥
        text = self.processor.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        image_inputs, video_inputs = process_vision_info(messages)
        inputs = self.processor(
            text=[text],
            images=image_inputs,
            videos=video_inputs,
            padding=True,
            return_tensors="pt",
        )
        inputs = inputs.to(self.device)
        
        # ç”Ÿæˆ
        generated_ids = self.model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            top_p=top_p,
            do_sample=do_sample
        )
        
        # è§£ç 
        generated_ids_trimmed = [
            out_ids[len(in_ids):] 
            for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
        ]
        output_text = self.processor.batch_decode(
            generated_ids_trimmed, 
            skip_special_tokens=True, 
            clean_up_tokenization_spaces=False
        )
        
        return output_text[0] if output_text else ""
    
    def parse_output(self, output_text: str) -> Dict[str, List[int]]:
        """è§£ææ¨¡å‹è¾“å‡ºä¸ºå¸ƒå±€å­—å…¸"""
        try:
            # æå–JSONéƒ¨åˆ†
            if "```json" in output_text:
                json_str = output_text.split("```json")[1].split("```")[0].strip()
            elif "```" in output_text:
                json_str = output_text.split("```")[1].split("```")[0].strip()
            else:
                json_str = output_text.strip()
            
            layout = json.loads(json_str)
            return layout
        except (json.JSONDecodeError, IndexError) as e:
            print(f"è§£æè¾“å‡ºå¤±è´¥: {e}")
            return {}
    
    def generate(
        self,
        image_path: str,
        query: str,
        existing_layout: Dict[str, List[int]] = None,
        config: GenerationConfig = None
    ) -> LayoutResult:
        """
        ç”Ÿæˆå¸ƒå±€å¹¶è§£æ
        
        Args:
            image_path: å›¾ç‰‡è·¯å¾„
            query: æŸ¥è¯¢æ–‡æœ¬
            existing_layout: å·²æœ‰å¸ƒå±€ï¼ˆç”¨äºè¯„ä¼°ï¼‰
            config: ç”Ÿæˆé…ç½®
            
        Returns:
            LayoutResult: ç”Ÿæˆç»“æœ
        """
        if config is None:
            config = GenerationConfig()
        
        # ç”ŸæˆåŸå§‹è¾“å‡º
        raw_output = self.generate_raw(
            image_path=image_path,
            query=query,
            max_new_tokens=config.max_new_tokens,
            temperature=config.temperature,
            top_p=config.top_p,
            do_sample=config.do_sample
        )
        
        # è§£æè¾“å‡º
        layout = self.parse_output(raw_output)
        
        # è¯„ä¼°ï¼ˆå¦‚æœæä¾›äº†å·²æœ‰å¸ƒå±€ï¼‰
        score = 0.0
        issues = []
        is_valid = bool(layout)
        
        if layout and existing_layout:
            eval_result = self.evaluator.evaluate(layout, existing_layout)
            score = eval_result.total_score
            issues = eval_result.issues
            is_valid = eval_result.is_valid
        
        return LayoutResult(
            layout=layout,
            raw_output=raw_output,
            score=score,
            is_valid=is_valid,
            issues=issues
        )
    
    def generate_candidates(
        self,
        image_path: str,
        query: str,
        existing_layout: Dict[str, List[int]] = None,
        num_candidates: int = 3,
        temperatures: List[float] = None
    ) -> List[LayoutResult]:
        """
        ç”Ÿæˆå¤šä¸ªå€™é€‰å¸ƒå±€
        
        Args:
            image_path: å›¾ç‰‡è·¯å¾„
            query: æŸ¥è¯¢æ–‡æœ¬
            existing_layout: å·²æœ‰å¸ƒå±€
            num_candidates: å€™é€‰æ•°é‡
            temperatures: æ¸©åº¦åˆ—è¡¨
            
        Returns:
            List[LayoutResult]: å€™é€‰ç»“æœåˆ—è¡¨
        """
        if temperatures is None:
            temperatures = [0.3, 0.5, 0.7, 0.9, 1.1][:num_candidates]
        
        candidates = []
        for temp in temperatures:
            config = GenerationConfig(temperature=temp)
            result = self.generate(
                image_path=image_path,
                query=query,
                existing_layout=existing_layout,
                config=config
            )
            candidates.append(result)
        
        return candidates
    
    def select_best(
        self,
        candidates: List[LayoutResult],
        existing_layout: Dict[str, List[int]]
    ) -> Tuple[LayoutResult, EvaluationResult]:
        """
        ä»å€™é€‰ä¸­é€‰æ‹©æœ€ä¼˜ç»“æœ
        
        Args:
            candidates: å€™é€‰åˆ—è¡¨
            existing_layout: å·²æœ‰å¸ƒå±€
            
        Returns:
            Tuple[æœ€ä¼˜ç»“æœ, è¯„ä¼°ç»“æœ]
        """
        best_result = None
        best_eval = None
        best_score = -1
        
        for candidate in candidates:
            if not candidate.layout:
                continue
            
            eval_result = self.evaluator.evaluate(candidate.layout, existing_layout)
            
            if eval_result.total_score > best_score:
                best_score = eval_result.total_score
                best_result = candidate
                best_eval = eval_result
        
        return best_result, best_eval
    
    def generate_optimized(
        self,
        image_path: str,
        query: str,
        existing_layout: Dict[str, List[int]],
        num_candidates: int = 5,
        score_threshold: float = 85.0,
        max_iterations: int = 3,
        auto_fix: bool = True,
        improvement_threshold: float = 3.0
    ) -> OptimizedResult:
        """
        å®Œæ•´ä¼˜åŒ–ç”Ÿæˆæµç¨‹ï¼š
        å¤šå€™é€‰ç”Ÿæˆ â†’ è¯„ä¼°æ‰“åˆ† â†’ é€‰æ‹©æœ€ä¼˜ â†’ è§„åˆ™ä¿®å¤ â†’ è¯†åˆ«é—®é¢˜ â†’ 
        æ³¨å…¥é—®é¢˜åˆ°Prompt â†’ é‡æ–°ç”Ÿæˆ â†’ å¾ªç¯ç›´åˆ°æ»¡æ„
        
        å®ç°ä¼˜åŒ–æŠ€æœ¯æ–¹æ¡ˆä¸­çš„è¿­ä»£ä¼˜åŒ–ç­–ç•¥ï¼š
        1. å¤šæ ·æ€§ç”Ÿæˆï¼šé€šè¿‡ä¸åŒæ¸©åº¦é‡‡æ ·äº§ç”Ÿå¤šä¸ªå€™é€‰
        2. è¯„åˆ†é€‰æ‹©ï¼šå¯¹å€™é€‰è¿›è¡Œäº”ç»´åº¦è¯„ä¼°ï¼Œé€‰æ‹©æœ€ä¼˜
        3. è§„åˆ™ä¿®å¤ï¼šå¯¹æœ€ä¼˜å€™é€‰è¿›è¡Œç¡¬æ€§è§„åˆ™ä¿®å¤
        4. è¿­ä»£ä¿®æ­£ï¼šå°†æœ¬è½®é—®é¢˜æ³¨å…¥Promptï¼Œå¼•å¯¼æ¨¡å‹åœ¨ä¸‹ä¸€è½®é¿å…
        
        Args:
            image_path: å›¾ç‰‡è·¯å¾„
            query: åŸå§‹æŸ¥è¯¢æ–‡æœ¬
            existing_layout: å·²æœ‰å¸ƒå±€å‚æ•°
            num_candidates: æ¯è½®å€™é€‰æ•°é‡
            score_threshold: æ»¡æ„åˆ†æ•°é˜ˆå€¼ï¼ˆè¾¾åˆ°ååœæ­¢ï¼‰
            max_iterations: æœ€å¤§è¿­ä»£è½®æ•°
            auto_fix: æ˜¯å¦ä½¿ç”¨è§„åˆ™å¼•æ“è‡ªåŠ¨ä¿®å¤
            improvement_threshold: æœ€å°æ”¹è¿›é˜ˆå€¼ï¼ˆä½äºæ­¤å€¼è§†ä¸ºæ”¶æ•›ï¼‰
            
        Returns:
            OptimizedResult: åŒ…å«å®Œæ•´ä¼˜åŒ–å†å²çš„ç»“æœ
        """
        best_layout = None
        best_raw_output = ""
        best_score = 0.0
        best_eval = None
        total_candidates = 0
        history = []
        
        current_query = query  # åˆå§‹æŸ¥è¯¢
        
        for iteration in range(max_iterations):
            iter_info = {
                'iteration': iteration + 1,
                'query_type': 'åˆå§‹æŸ¥è¯¢' if iteration == 0 else 'ä¿®æ­£æŸ¥è¯¢',
            }
            
            print(f"\n{'='*50}")
            print(f"ğŸ”„ ç¬¬ {iteration + 1}/{max_iterations} è½®ä¼˜åŒ–")
            print(f"{'='*50}")
            
            # ========== ç¬¬1æ­¥ï¼šå¤šå€™é€‰ç”Ÿæˆ ==========
            print(f"  ğŸ“ ç”Ÿæˆ {num_candidates} ä¸ªå€™é€‰...")
            candidates = self.generate_candidates(
                image_path=image_path,
                query=current_query,
                existing_layout=existing_layout,
                num_candidates=num_candidates
            )
            total_candidates += len(candidates)
            iter_info['num_candidates'] = len(candidates)
            
            # ========== ç¬¬2æ­¥ï¼šè¯„ä¼°æ‰“åˆ† + éªŒè¯ ==========
            print(f"  ğŸ” è¯„ä¼°å€™é€‰ç»“æœ...")
            candidate_details = []
            for i, cand in enumerate(candidates):
                if not cand.layout:
                    print(f"    å€™é€‰{i+1}: âŒ è§£æå¤±è´¥")
                    continue
                
                eval_result = self.evaluator.evaluate(cand.layout, existing_layout)
                validation = self.rule_engine.validate(cand.layout, existing_layout)
                
                candidate_details.append({
                    'index': i,
                    'layout': cand.layout,
                    'raw_output': cand.raw_output,
                    'score': eval_result.total_score,
                    'evaluation': eval_result,
                    'validation': validation,
                    'is_rule_valid': validation.valid
                })
                
                status = "âœ…" if validation.valid else "âš ï¸"
                print(f"    å€™é€‰{i+1}: {status} å¾—åˆ†={eval_result.total_score:.1f}, "
                      f"è§„åˆ™é€šè¿‡={validation.valid}")
            
            iter_info['num_valid'] = sum(
                1 for c in candidate_details if c['is_rule_valid']
            )
            
            if not candidate_details:
                print(f"  âš ï¸ æœ¬è½®æ— æœ‰æ•ˆå€™é€‰")
                iter_info['best_score'] = 0
                iter_info['issues'] = ['æ‰€æœ‰å€™é€‰å‡è§£æå¤±è´¥']
                history.append(iter_info)
                continue
            
            # ========== ç¬¬3æ­¥ï¼šé€‰æ‹©æœ€ä¼˜å€™é€‰ ==========
            # ä¼˜å…ˆé€‰æ‹©é€šè¿‡ç¡¬æ€§è§„åˆ™éªŒè¯çš„
            valid_candidates = [c for c in candidate_details if c['is_rule_valid']]
            pool = valid_candidates if valid_candidates else candidate_details
            round_best = max(pool, key=lambda x: x['score'])
            
            print(f"  ğŸ† æœ¬è½®æœ€ä¼˜: å€™é€‰{round_best['index']+1}, "
                  f"å¾—åˆ†={round_best['score']:.1f}")
            
            iter_info['best_score'] = round_best['score']
            iter_info['issues'] = round_best['evaluation'].issues
            
            # ========== ç¬¬4æ­¥ï¼šè§„åˆ™å¼•æ“ä¿®å¤ ==========
            round_layout = round_best['layout']
            round_raw = round_best['raw_output']
            round_eval = round_best['evaluation']
            
            if auto_fix and not round_best['is_rule_valid']:
                print(f"  ğŸ”§ è§„åˆ™å¼•æ“ä¿®å¤ä¸­...")
                fix_result = self.rule_engine.validate_and_fix(
                    round_layout, existing_layout
                )
                if fix_result.fixed_layout:
                    round_layout = fix_result.fixed_layout
                    round_eval = self.evaluator.evaluate(
                        round_layout, existing_layout
                    )
                    print(f"    ä¿®å¤åå¾—åˆ†: {round_eval.total_score:.1f}")
                    iter_info['fixed_score'] = round_eval.total_score
            
            # ========== ç¬¬5æ­¥ï¼šæ›´æ–°å…¨å±€æœ€ä¼˜ ==========
            if round_eval.total_score > best_score:
                improvement = round_eval.total_score - best_score
                best_layout = round_layout
                best_raw_output = round_raw
                best_score = round_eval.total_score
                best_eval = round_eval
                print(f"  â¬†ï¸ å…¨å±€æœ€ä¼˜æ›´æ–°: {best_score:.1f} (+{improvement:.1f})")
                iter_info['improvement'] = improvement
            else:
                print(f"  â¡ï¸ å…¨å±€æœ€ä¼˜æœªå˜: {best_score:.1f}")
                iter_info['improvement'] = 0
            
            history.append(iter_info)
            
            # ========== ç¬¬6æ­¥ï¼šæ£€æŸ¥ç»ˆæ­¢æ¡ä»¶ ==========
            if best_score >= score_threshold:
                print(f"  âœ… è¾¾åˆ°æ»¡æ„é˜ˆå€¼ ({score_threshold}), åœæ­¢ä¼˜åŒ–")
                break
            
            # æ£€æŸ¥æ”¶æ•›
            if iteration > 0 and iter_info.get('improvement', 0) < improvement_threshold:
                print(f"  ğŸ“‰ æ”¹è¿›å¹…åº¦ä¸è¶³ ({iter_info.get('improvement', 0):.1f} < {improvement_threshold}), åœæ­¢ä¼˜åŒ–")
                break
            
            # ========== ç¬¬7æ­¥ï¼šæ„é€ ä¿®æ­£Prompt ==========
            if iteration < max_iterations - 1 and round_eval.issues:
                current_query = self._build_fix_query(
                    original_query=query,
                    current_layout=round_layout,
                    issues=round_eval.issues
                )
                print(f"  ğŸ“‹ å·²æ³¨å…¥ {len(round_eval.issues)} ä¸ªé—®é¢˜åˆ°ä¸‹ä¸€è½®Prompt")
        
        # ========== æœ€ç»ˆç»“æœ ==========
        if best_layout is None:
            return OptimizedResult(
                layout={},
                raw_output="",
                score=0,
                is_satisfactory=False,
                issues=["æ‰€æœ‰è½®æ¬¡å‡æœªç”Ÿæˆæœ‰æ•ˆå¸ƒå±€"],
                suggestions=["è¯·æ£€æŸ¥è¾“å…¥å‚æ•°å’Œå›¾ç‰‡è·¯å¾„"],
                candidates_count=total_candidates,
                optimization_rounds=len(history),
                iteration_history=history
            )
        
        # æœ€ç»ˆè§„åˆ™ä¿®å¤
        if auto_fix:
            final_fix = self.rule_engine.validate_and_fix(best_layout, existing_layout)
            if final_fix.fixed_layout:
                best_layout = final_fix.fixed_layout
                best_eval = self.evaluator.evaluate(best_layout, existing_layout)
        
        print(f"\n{'='*50}")
        print(f"ğŸ¯ ä¼˜åŒ–å®Œæˆ!")
        print(f"  æœ€ç»ˆå¾—åˆ†: {best_eval.total_score:.1f}")
        print(f"  æ€»å€™é€‰æ•°: {total_candidates}")
        print(f"  è¿­ä»£è½®æ•°: {len(history)}")
        print(f"  æ˜¯å¦æ»¡æ„: {best_eval.total_score >= score_threshold}")
        if best_eval.issues:
            print(f"  å‰©ä½™é—®é¢˜: {len(best_eval.issues)} ä¸ª")
        print(f"{'='*50}")
        
        return OptimizedResult(
            layout=best_layout,
            raw_output=best_raw_output,
            score=best_eval.total_score,
            is_satisfactory=best_eval.total_score >= score_threshold,
            issues=best_eval.issues,
            suggestions=best_eval.suggestions,
            candidates_count=total_candidates,
            optimization_rounds=len(history),
            iteration_history=history
        )
    
    def _build_fix_query(
        self,
        original_query: str,
        current_layout: Dict[str, List[int]],
        issues: List[str]
    ) -> str:
        """
        æ„é€ è¿­ä»£ä¿®æ­£æŸ¥è¯¢ï¼šå°†ä¸Šä¸€è½®çš„é—®é¢˜æ³¨å…¥Prompt
        å¼•å¯¼æ¨¡å‹åœ¨ä¸‹ä¸€æ¬¡ç”Ÿæˆæ—¶é¿å…è¿™äº›é—®é¢˜
        
        å¯¹åº”ä¼˜åŒ–æŠ€æœ¯æ–¹æ¡ˆä¸­çš„ "è¿­ä»£ä¼˜åŒ–æµç¨‹"ï¼š
        ç”Ÿæˆåˆå§‹å¸ƒå±€ â†’ è¯„ä¼°æ‰“åˆ† â†’ è¯†åˆ«é—®é¢˜ â†’ é’ˆå¯¹æ€§ä¿®æ­£ â†’ å¾ªç¯
        """
        issues_text = "\n".join(f"  - {issue}" for issue in issues)
        layout_json = json.dumps(current_layout, ensure_ascii=False, indent=2)
        
        # å°è¯•ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„fix_promptæ¨¡æ¿
        fix_template = self.prompts_config.get('fix_prompt', '')
        if fix_template and '{issues}' in fix_template:
            fix_section = fix_template.format(
                issues=issues_text,
                original_layout=layout_json
            )
        else:
            fix_section = (
                f"\næ³¨æ„ï¼šä¸Šä¸€æ¬¡ç”Ÿæˆçš„å¸ƒå±€å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼Œè¯·åœ¨æœ¬æ¬¡ç”Ÿæˆä¸­é¿å…ï¼š\n"
                f"{issues_text}\n\n"
                f"ä¸Šä¸€æ¬¡çš„å¸ƒå±€ï¼ˆä»…ä¾›å‚è€ƒï¼Œéœ€è¦æ”¹è¿›ï¼‰ï¼š\n"
                f"```json\n{layout_json}\n```\n\n"
                f"è¯·ç”Ÿæˆä¸€ä¸ªæ”¹è¿›åçš„å¸ƒå±€ï¼Œè§£å†³ä¸Šè¿°é—®é¢˜ã€‚"
            )
        
        return f"{original_query}\n{fix_section}"
    
    def evaluate(
        self,
        layout: Dict[str, List[int]],
        existing_layout: Dict[str, List[int]]
    ) -> EvaluationResult:
        """è¯„ä¼°å¸ƒå±€"""
        return self.evaluator.evaluate(layout, existing_layout)
    
    def validate(
        self,
        layout: Dict[str, List[int]],
        existing_layout: Dict[str, List[int]],
        auto_fix: bool = False
    ) -> ValidationResult:
        """éªŒè¯å¸ƒå±€"""
        if auto_fix:
            return self.rule_engine.validate_and_fix(layout, existing_layout)
        return self.rule_engine.validate(layout, existing_layout)


def build_query(
    house_type: str,
    floor_type: str,
    existing_params: Dict[str, List[int]],
    rooms_to_generate: List[str],
    design_constraints: str = None,
    prompts_config: Dict = None
) -> str:
    """
    æ„å»ºæŸ¥è¯¢æ–‡æœ¬ï¼ˆå¢å¼ºç‰ˆï¼Œå¯æ³¨å…¥è®¾è®¡çº¦æŸï¼‰
    
    Args:
        house_type: ä½å®…ç±»å‹ï¼ˆåŸå¸‚/ä¹¡æ‘ï¼‰
        floor_type: æ¥¼å±‚ç±»å‹ï¼ˆä¸€å±‚/äºŒå±‚ç­‰ï¼‰
        existing_params: å·²æœ‰å‚æ•°
        rooms_to_generate: å¾…ç”Ÿæˆçš„æˆ¿é—´åˆ—è¡¨
        design_constraints: è®¾è®¡çº¦æŸæ–‡æœ¬ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä»é…ç½®åŠ è½½ï¼‰
        prompts_config: æç¤ºè¯é…ç½®å­—å…¸ï¼ˆå¯é€‰ï¼‰
        
    Returns:
        æŸ¥è¯¢æ–‡æœ¬
    """
    existing_json = json.dumps(existing_params, ensure_ascii=False)
    rooms_json = json.dumps(rooms_to_generate, ensure_ascii=False)
    
    # è·å–è®¾è®¡çº¦æŸ
    if design_constraints is None and prompts_config:
        design_constraints = prompts_config.get('design_constraints', '')
    elif design_constraints is None:
        # å°è¯•ä»é…ç½®æ–‡ä»¶åŠ è½½
        config_path = Path(__file__).parent / 'config' / 'prompts.yaml'
        if config_path.exists():
            try:
                with open(config_path, 'r', encoding='utf-8') as f:
                    config = yaml.safe_load(f)
                design_constraints = config.get('design_constraints', '')
            except Exception:
                design_constraints = ''
    
    # æ„å»ºå¸¦çº¦æŸçš„æŸ¥è¯¢
    constraints_section = ""
    if design_constraints:
        constraints_section = f"\n{design_constraints.strip()}\n"
    
    query = f'''è¯·æ ¹æ®è¿™å¼ å›¾ç‰‡ä¸­å·²æœ‰çš„æˆ·å‹ä¿¡æ¯ä»¥åŠå¯¹åº”çš„å‚æ•°ï¼Œå¸®æˆ‘ç”Ÿæˆå…¶ä½™æˆ¿é—´çš„å‚æ•°ï¼Œå¾—åˆ°ä¸€ä¸ªå®Œæ•´çš„åˆç†å¹³é¢å¸ƒå±€ã€‚æ„æˆæˆ·å‹çš„æ‰€æœ‰ç©ºé—´å•å…ƒå‡è¡¨ç¤ºä¸ºçŸ©å½¢ï¼Œç”¨xè½´åæ ‡ã€yè½´åæ ‡ã€é•¿åº¦ã€å®½åº¦å››ä¸ªå‚æ•°è¡¨ç¤ºã€‚æœ¬æˆ·å‹ä¸º"{house_type}"ä½å®…ï¼Œå›¾ç‰‡ä¸­çš„ä¸º"{floor_type}"å¹³é¢ã€‚
{constraints_section}
å›¾ç‰‡ä¸­å·²æœ‰ä¿¡æ¯å¯¹åº”çš„å‚æ•°ä¸ºï¼š
```json
{existing_json}
```å…¶ä½™å¾…ç”Ÿæˆçš„"{floor_type}"æˆ¿é—´çš„åç§°ä¸ºï¼š
```json
{rooms_json}```'''
    
    return query


# ä¾¿æ·å‡½æ•°
def create_predictor(
    base_model_path: str = "models/Qwen2.5-VL-7B-Instruct",
    lora_adapter_path: str = "lora_model",
    **kwargs
) -> LayoutPredictor:
    """åˆ›å»ºé¢„æµ‹å™¨å®ä¾‹"""
    return LayoutPredictor(
        base_model_path=base_model_path,
        lora_adapter_path=lora_adapter_path,
        **kwargs
    )


if __name__ == "__main__":
    # æµ‹è¯•ï¼ˆä¸åŠ è½½æ¨¡å‹ï¼Œä»…æµ‹è¯•è¯„ä¼°åŠŸèƒ½ï¼‰
    print("æµ‹è¯• LayoutPredictorï¼ˆè¯„ä¼°åŠŸèƒ½ï¼‰...")
    
    predictor = LayoutPredictor()
    
    # æµ‹è¯•è¯„ä¼°
    existing = {
        "è¾¹ç•Œ": [0, 0, 9600, 10500],
        "å—é‡‡å…‰": [0, -1200, 9600, 1200],
    }
    
    generated = {
        "å®¢å…": [0, 0, 4000, 4000],
        "å§å®¤1": [0, 4500, 3300, 4000],
        "å¨æˆ¿": [4500, 0, 2400, 3000],
    }
    
    result = predictor.evaluate(generated, existing)
    print(f"è¯„ä¼°å¾—åˆ†: {result.total_score:.1f}")
    print(f"é—®é¢˜: {result.issues}")
    
    # æµ‹è¯•éªŒè¯
    val_result = predictor.validate(generated, existing)
    print(f"éªŒè¯é€šè¿‡: {val_result.valid}")
    
    # æµ‹è¯•æŸ¥è¯¢æ„å»º
    query = build_query(
        house_type="åŸå¸‚",
        floor_type="ä¸€å±‚",
        existing_params=existing,
        rooms_to_generate=["å®¢å…", "å§å®¤1", "å¨æˆ¿"]
    )
    print(f"\næ„å»ºçš„æŸ¥è¯¢:\n{query[:200]}...")
    
    print("\næµ‹è¯•å®Œæˆ!")
